{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **02 Variables **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the last session, we learned about the difference between tf.Variables and tf.Tensor. In this chapter, we are going to review in-depth use of tf.Variable and torch.autograd.Variable.\n",
    "\n",
    "\n",
    "# 1. Creating a Variable\n",
    "## **[TensorFlow]** tf.get_variable() function or tf.Variable\n",
    "Unlike tf.Tensor objects, a tf.Variable exists outside the context of a single session.run call.\n",
    "\n",
    "And Variable in TensorFlow is one of the most important concept that you should always well aware of to build your own networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** - Method 1:** tf.get_variable()  \n",
    "\n",
    "To get the Tensorflow Variable, we can use get_variable function. There are many arguments for get_variable function but usually [name], [dtype], [initializer]. Without dtype, it automatically sets the dtype as \"tf.float32\". (This is different from python3 numpy, which uses float64 as a default dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'tf_variable' not in locals(): # To prevent overwriting problem.\n",
    "    tf_variable = tf.get_variable('tensorflow_variable', [1, 2, 3])\n",
    "    tf_variable_int = tf.get_variable('tensorflow_int_var', [1, 2, 3], dtype=tf.int32)\n",
    "    tf_variable_intialized = tf.get_variable('tensorflow_var_init', [1, 2, 3], dtype=tf.int32, initializer=tf.zeros_initializer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Tensorflow variable has \"name\" propoerty, if you declare a Tensorflow variable with same name, you get an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, tf.Variable can be initialized with tf.constant, which is tf.Tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_variable has the type of : <class 'tensorflow.python.ops.variables.Variable'> and the shape of : (2,)\n"
     ]
    }
   ],
   "source": [
    "if 'tf_variable_constintialized' not in locals():\n",
    "    tf_variable_constintialized = tf.get_variable('tensorflow_var_init_const', dtype=tf.int32, initializer=tf.constant([1,2]))\n",
    "print('tf_variable has the type of :', type(tf_variable_constintialized), 'and the shape of :', tf_variable_constintialized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** - Method 2:** tf.Variable\n",
    "\n",
    "We can also use tf.Variable() to create TensorFlow variables. This method is similar to pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_weigts has type of : <class 'tensorflow.python.ops.variables.Variable'> and shape of (256, 100)\n",
      "tf_biases has type of : <class 'tensorflow.python.ops.variables.Variable'> and shape of (100,)\n"
     ]
    }
   ],
   "source": [
    "tf_weights = tf.Variable(tf.random_normal([256, 100], stddev=0.35), name=\"tf_weights\")\n",
    "tf_biases = tf.Variable(tf.zeros([100]), name=\"tf_biases\")\n",
    "weprint('tf_biases has type of :', type(tf_biases), 'and shape of', tf_biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **[PyTorch]** Creating PyTorch Variable - torch.autograd.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned from the last chapter, Pytorch is based on the concept of \"Variable\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Variable x:  Variable containing:\n",
      " 1\n",
      " 2\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_np = np.array([1, 2])\n",
    "x = torch.autograd.Variable(torch.from_numpy(x_np).type(torch.FloatTensor), requires_grad=True)\n",
    "print('Torch Variable x: ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, most of the time, Pytorch users use the following convention for the torch.autograd.Variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Variable x:  Variable containing:\n",
      "-0.0094 -2.2096\n",
      " 0.5326  0.5228\n",
      " 0.6303  0.4982\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I recommand you to use this convention so that you can read other people's codes.\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(3, 2).type(torch.FloatTensor), requires_grad=False)\n",
    "print('Torch Variable x: ', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The concept of Pytorch Variable\n",
    "Unlike tf.Variable, torch.Variable needs a lot more explanations. \n",
    "\n",
    "PyTorch's Variable contains three different entities as below\n",
    "\n",
    "**torch.autograd.Variable**\n",
    "> **data**: Raw data Variable contains inside the variable.\n",
    "\n",
    "> **grad**: Gradient obtained from Autograd feature in PyTorch.\n",
    "\n",
    "> **creator**: Variable remembers how the variable is created and what operation it has gone through. \n",
    "(*Creator does not exists as a real variable in the torch.autograd.Variable.)\n",
    "\n",
    "\n",
    "Unlike TensorFlow, PyTorch Variable contains the history of the Variable itself to enable Autograd feature. When the a variable is declared, .grad and .grad_fn contain None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.data: \n",
      "-0.0094 -2.2096\n",
      " 0.5326  0.5228\n",
      " 0.6303  0.4982\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "x.grad: None\n",
      "x.grad: None\n"
     ]
    }
   ],
   "source": [
    "print('x.data:', x.data)\n",
    "print('x.grad:', x.grad)\n",
    "print('x.grad:', x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the Variables go through some mathmatical operation and we use .backward() function to use Autograd feature, we can see what is inside the variables .data, .grad and .grad_fn. \n",
    "\n",
    "\".grad_fn\" variable contains the gradiemt function that has automatically assigned to the operation. \n",
    "\n",
    "We will discuss about this in detail in computation chapter. Here, make sure you understand torch.autograd.Variable contains the following variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y, z contains : Variable containing:\n",
      "-0.8405 -3.5112\n",
      " 6.2653  2.5572\n",
      "-0.7553 -3.0431\n",
      "[torch.FloatTensor of size 3x2]\n",
      " \n",
      " Variable containing:\n",
      " 0.1121\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "After backward() x.data: \n",
      "-0.4202 -1.7556\n",
      " 3.1327  1.2786\n",
      "-0.3776 -1.5216\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n",
      "After backward() x.grad: Variable containing:\n",
      " 0.3333  0.3333\n",
      " 0.3333  0.3333\n",
      " 0.3333  0.3333\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(3, 2).type(torch.FloatTensor), requires_grad=True)\n",
    "y = x * 2\n",
    "z = y.mean()\n",
    "print('y, z contains :', y, '\\n', z)\n",
    "z.backward()\n",
    "print('After backward() x.data:', x.data)\n",
    "print('After backward() x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, after excuting .backward() function, .grad_fn variables are assigned with gradient function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad_fn: None\n",
      "y.grad_fn: <torch.autograd.function.MulConstantBackward object at 0x7f722c164318>\n",
      "z.grad_fn: <torch.autograd.function.MeanBackward object at 0x7f722c164228>\n"
     ]
    }
   ],
   "source": [
    "print('x.grad_fn:', x.grad_fn)\n",
    "print('y.grad_fn:', y.grad_fn)\n",
    "print('z.grad_fn:', z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, x is not assigned with grad_fn because we started the operation from x. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
